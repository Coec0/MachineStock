{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "input_size=500\n",
    "batch_size=512\n",
    "nbr_epochs=5\n",
    "data_split_ratio=0.8\n",
    "chunksize = 100000\n",
    "lr = 0.0001\n",
    "y_column = \"600s\"\n",
    "files_x = [\"data/x_Swedbank_A_500_p_norm.csv\",]\n",
    "files_y = [\"data/y_Swedbank_A_500_norm.csv\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(xs, ys, trainRatio):\n",
    "    t = round(len(xs)*trainRatio)\n",
    "    \n",
    "    train_data_x = torch.tensor(xs[:t].values, dtype=torch.float32)\n",
    "    train_data_y = torch.tensor(ys[:t].values, dtype=torch.float32)\n",
    "    \n",
    "    dev_data_x = torch.tensor(xs[t:].values, dtype=torch.float32)\n",
    "    dev_data_y = torch.tensor(ys[t:].values, dtype=torch.float32)\n",
    "    \n",
    "    return TensorDataset(train_data_x, train_data_y), TensorDataset(dev_data_x, dev_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_unit = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device(\"cpu\")\n",
    "#compute_unit = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1000).type(dtype)\n",
    "        self.fc1.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc2 = nn.Linear(1000, 2000).type(dtype)\n",
    "        self.fc2.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc3 = nn.Linear(2000, 1000).type(dtype)\n",
    "        self.fc3.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc4 = nn.Linear(1000, 500).type(dtype)\n",
    "        self.fc4.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc5 = nn.Linear(500, 100).type(dtype)\n",
    "        self.fc5.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc6 = nn.Linear(100, 20).type(dtype)\n",
    "        self.fc6.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc7 = nn.Linear(20, 1).type(dtype)\n",
    "        self.fc7.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(num_features=input_size, track_running_stats=True)\n",
    "        #self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.drop_layer = nn.Dropout(p=0.010)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x = self.bn1(x)\n",
    "        fc1 = self.fc1(x)\n",
    "        x = F.leaky_relu(fc1)\n",
    "        #x = self.drop_layer(x)\n",
    "        #print(fc1)\n",
    "        #print(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        #x = self.drop_layer(x)\n",
    "        #print(x)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        #x = self.drop_layer(x)\n",
    "        #x = self.drop_layer(x)\n",
    "        #print(x)\n",
    "        x= F.leaky_relu(self.fc4(x))\n",
    "        #print(y)\n",
    "        x = F.leaky_relu(self.fc5(x))\n",
    "        #x = self.drop_layer(x)\n",
    "        x = F.leaky_relu(self.fc6(x))\n",
    "        y = F.leaky_relu(self.fc7(x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data, model, loss_fn):\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data:\n",
    "            y = y.type(dtype)\n",
    "            x = x.type(dtype)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            losses.append(loss.item())\n",
    "            predictions.extend(pred.tolist())\n",
    "        avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return avg_loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data_loader, dev_data_loader, loss_fn, optimizer, epochrange, batchsize):\n",
    "    for epoch in range(epochrange):\n",
    "        losses = []\n",
    "        n_correct = 0\n",
    "        model.train()\n",
    "        for x, y in train_data_loader:\n",
    "            y = y.type(dtype)\n",
    "            x = x.type(dtype)\n",
    "            \n",
    "            pred = model(x)\n",
    "\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()    \n",
    "\n",
    "        # Compute accuracy and loss in the entire training set\n",
    "        train_avg_loss = sum(losses)/len(losses)    \n",
    "        \n",
    "        dev_avg_loss,_ = evaluate_model(dev_data_loader, model, loss_fn)\n",
    "        \n",
    "        # Display metrics\n",
    "        display_str = 'Epoch {} '\n",
    "        display_str += '\\tLoss: {:.6f} '\n",
    "        display_str += '\\tLoss (val): {:.6f}'\n",
    "        print(display_str.format(epoch, train_avg_loss, dev_avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_chunk(model, loss_fn, optimizer, nbr_epochs, x_data, y_data, data_split_ratio, batch_size):\n",
    "    train_data, dev_data = splitData(x_data, y_data, data_split_ratio)\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    dev_data_loader = DataLoader(dev_data, batch_size=batch_size)\n",
    "    train_model(model, train_data_loader, dev_data_loader, loss_fn, optimizer, nbr_epochs, batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file: data/x_Swedbank_A_500_p_norm.csv\n",
      "Number of chunks: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/anaconda3/envs/gtx/lib/python3.7/site-packages/ipykernel_launcher.py:16: ParserWarning: Both a converter and dtype were specified for column ts - only the converter will be used\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/anaconda3/envs/gtx/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/oscar/anaconda3/envs/gtx/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/oscar/anaconda3/envs/gtx/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \tLoss: 0.330797 \tLoss (val): 0.493132\n",
      "Epoch 1 \tLoss: 0.304238 \tLoss (val): 0.390385\n",
      "Epoch 2 \tLoss: 0.288654 \tLoss (val): 0.353124\n",
      "Epoch 3 \tLoss: 0.282844 \tLoss (val): 0.337697\n",
      "Epoch 4 \tLoss: 0.278275 \tLoss (val): 0.319950\n",
      "Progress: 3.23%\n",
      "Epoch 0 \tLoss: 0.229780 \tLoss (val): 0.198181\n",
      "Epoch 1 \tLoss: 0.212917 \tLoss (val): 0.200325\n",
      "Epoch 2 \tLoss: 0.207630 \tLoss (val): 0.201842\n",
      "Epoch 3 \tLoss: 0.204066 \tLoss (val): 0.201323\n",
      "Epoch 4 \tLoss: 0.201509 \tLoss (val): 0.202964\n",
      "Progress: 6.45%\n",
      "Epoch 0 \tLoss: 0.250700 \tLoss (val): 0.189680\n",
      "Epoch 1 \tLoss: 0.240979 \tLoss (val): 0.187865\n",
      "Epoch 2 \tLoss: 0.234088 \tLoss (val): 0.188939\n",
      "Epoch 3 \tLoss: 0.228610 \tLoss (val): 0.192675\n",
      "Epoch 4 \tLoss: 0.223268 \tLoss (val): 0.200097\n",
      "Progress: 9.68%\n",
      "Epoch 0 \tLoss: 0.277867 \tLoss (val): 0.158431\n",
      "Epoch 1 \tLoss: 0.275272 \tLoss (val): 0.163577\n",
      "Epoch 2 \tLoss: 0.269880 \tLoss (val): 0.164857\n",
      "Epoch 3 \tLoss: 0.266506 \tLoss (val): 0.167878\n",
      "Epoch 4 \tLoss: 0.264565 \tLoss (val): 0.170028\n",
      "Progress: 12.90%\n",
      "Epoch 0 \tLoss: 0.286505 \tLoss (val): 0.160407\n",
      "Epoch 1 \tLoss: 0.271446 \tLoss (val): 0.160181\n",
      "Epoch 2 \tLoss: 0.268119 \tLoss (val): 0.159668\n",
      "Epoch 3 \tLoss: 0.264832 \tLoss (val): 0.159347\n",
      "Epoch 4 \tLoss: 0.261847 \tLoss (val): 0.159215\n",
      "Progress: 16.13%\n",
      "Epoch 0 \tLoss: 0.469233 \tLoss (val): 0.244740\n",
      "Epoch 1 \tLoss: 0.463030 \tLoss (val): 0.247318\n",
      "Epoch 2 \tLoss: 0.459586 \tLoss (val): 0.248149\n",
      "Epoch 3 \tLoss: 0.456472 \tLoss (val): 0.248735\n",
      "Epoch 4 \tLoss: 0.452908 \tLoss (val): 0.249838\n",
      "Progress: 19.35%\n",
      "Epoch 0 \tLoss: 0.156694 \tLoss (val): 0.134178\n",
      "Epoch 1 \tLoss: 0.148969 \tLoss (val): 0.135246\n",
      "Epoch 2 \tLoss: 0.144923 \tLoss (val): 0.137355\n",
      "Epoch 3 \tLoss: 0.141386 \tLoss (val): 0.139417\n",
      "Epoch 4 \tLoss: 0.138537 \tLoss (val): 0.141317\n",
      "Progress: 22.58%\n",
      "Epoch 0 \tLoss: 0.144596 \tLoss (val): 0.133744\n",
      "Epoch 1 \tLoss: 0.135281 \tLoss (val): 0.132707\n",
      "Epoch 2 \tLoss: 0.130930 \tLoss (val): 0.132072\n",
      "Epoch 3 \tLoss: 0.127559 \tLoss (val): 0.132511\n",
      "Epoch 4 \tLoss: 0.123753 \tLoss (val): 0.133005\n",
      "Progress: 25.81%\n",
      "Epoch 0 \tLoss: 0.156817 \tLoss (val): 0.111138\n",
      "Epoch 1 \tLoss: 0.144514 \tLoss (val): 0.109748\n",
      "Epoch 2 \tLoss: 0.135760 \tLoss (val): 0.111051\n",
      "Epoch 3 \tLoss: 0.131437 \tLoss (val): 0.112662\n",
      "Epoch 4 \tLoss: 0.127066 \tLoss (val): 0.116302\n",
      "Progress: 29.03%\n",
      "Epoch 0 \tLoss: 0.186966 \tLoss (val): 0.076169\n",
      "Epoch 1 \tLoss: 0.172394 \tLoss (val): 0.077854\n",
      "Epoch 2 \tLoss: 0.168905 \tLoss (val): 0.079623\n",
      "Epoch 3 \tLoss: 0.165506 \tLoss (val): 0.081656\n",
      "Epoch 4 \tLoss: 0.162691 \tLoss (val): 0.083423\n",
      "Progress: 32.26%\n",
      "Epoch 0 \tLoss: 0.190063 \tLoss (val): 0.197655\n",
      "Epoch 1 \tLoss: 0.185193 \tLoss (val): 0.191701\n",
      "Epoch 2 \tLoss: 0.178255 \tLoss (val): 0.192566\n",
      "Epoch 3 \tLoss: 0.174080 \tLoss (val): 0.191385\n",
      "Epoch 4 \tLoss: 0.169632 \tLoss (val): 0.189240\n",
      "Progress: 35.48%\n",
      "Epoch 0 \tLoss: 0.442015 \tLoss (val): 0.175981\n",
      "Epoch 1 \tLoss: 0.421838 \tLoss (val): 0.175668\n",
      "Epoch 2 \tLoss: 0.414354 \tLoss (val): 0.175743\n",
      "Epoch 3 \tLoss: 0.409790 \tLoss (val): 0.175946\n",
      "Epoch 4 \tLoss: 0.407279 \tLoss (val): 0.176285\n",
      "Progress: 38.71%\n",
      "Epoch 0 \tLoss: 0.336952 \tLoss (val): 0.205603\n",
      "Epoch 1 \tLoss: 0.330158 \tLoss (val): 0.204132\n",
      "Epoch 2 \tLoss: 0.326389 \tLoss (val): 0.205484\n",
      "Epoch 3 \tLoss: 0.324440 \tLoss (val): 0.208667\n",
      "Epoch 4 \tLoss: 0.321799 \tLoss (val): 0.208574\n",
      "Progress: 41.94%\n",
      "Epoch 0 \tLoss: 0.246090 \tLoss (val): 0.139434\n",
      "Epoch 1 \tLoss: 0.239912 \tLoss (val): 0.142179\n",
      "Epoch 2 \tLoss: 0.236810 \tLoss (val): 0.142907\n",
      "Epoch 3 \tLoss: 0.234484 \tLoss (val): 0.145901\n",
      "Epoch 4 \tLoss: 0.231365 \tLoss (val): 0.147963\n",
      "Progress: 45.16%\n",
      "Epoch 0 \tLoss: 0.191606 \tLoss (val): 0.128036\n",
      "Epoch 1 \tLoss: 0.183685 \tLoss (val): 0.127086\n",
      "Epoch 2 \tLoss: 0.181626 \tLoss (val): 0.126588\n",
      "Epoch 3 \tLoss: 0.180075 \tLoss (val): 0.125917\n",
      "Epoch 4 \tLoss: 0.178890 \tLoss (val): 0.126438\n",
      "Progress: 48.39%\n",
      "Epoch 0 \tLoss: 0.382317 \tLoss (val): 0.666681\n",
      "Epoch 1 \tLoss: 0.373760 \tLoss (val): 0.669517\n",
      "Epoch 2 \tLoss: 0.369562 \tLoss (val): 0.672615\n",
      "Epoch 3 \tLoss: 0.366607 \tLoss (val): 0.676042\n",
      "Epoch 4 \tLoss: 0.364013 \tLoss (val): 0.679111\n",
      "Progress: 51.61%\n",
      "Epoch 0 \tLoss: 0.386828 \tLoss (val): 0.267545\n",
      "Epoch 1 \tLoss: 0.362837 \tLoss (val): 0.269104\n",
      "Epoch 2 \tLoss: 0.357620 \tLoss (val): 0.271968\n",
      "Epoch 3 \tLoss: 0.353599 \tLoss (val): 0.270956\n",
      "Epoch 4 \tLoss: 0.349234 \tLoss (val): 0.273886\n",
      "Progress: 54.84%\n",
      "Epoch 0 \tLoss: 0.244984 \tLoss (val): 0.475363\n",
      "Epoch 1 \tLoss: 0.236782 \tLoss (val): 0.475792\n",
      "Epoch 2 \tLoss: 0.233331 \tLoss (val): 0.480019\n",
      "Epoch 3 \tLoss: 0.230008 \tLoss (val): 0.481152\n",
      "Epoch 4 \tLoss: 0.227961 \tLoss (val): 0.482585\n",
      "Progress: 58.06%\n",
      "Epoch 0 \tLoss: 0.167460 \tLoss (val): 0.335102\n",
      "Epoch 1 \tLoss: 0.159054 \tLoss (val): 0.341824\n",
      "Epoch 2 \tLoss: 0.156839 \tLoss (val): 0.344808\n",
      "Epoch 3 \tLoss: 0.154710 \tLoss (val): 0.347038\n",
      "Epoch 4 \tLoss: 0.152471 \tLoss (val): 0.349620\n",
      "Progress: 61.29%\n",
      "Epoch 0 \tLoss: 0.201081 \tLoss (val): 0.169704\n",
      "Epoch 1 \tLoss: 0.195035 \tLoss (val): 0.170846\n",
      "Epoch 2 \tLoss: 0.192558 \tLoss (val): 0.171845\n",
      "Epoch 3 \tLoss: 0.189623 \tLoss (val): 0.172565\n",
      "Epoch 4 \tLoss: 0.187475 \tLoss (val): 0.173440\n",
      "Progress: 64.52%\n",
      "Epoch 0 \tLoss: 0.184452 \tLoss (val): 0.152628\n",
      "Epoch 1 \tLoss: 0.176538 \tLoss (val): 0.151113\n",
      "Epoch 2 \tLoss: 0.172694 \tLoss (val): 0.152329\n",
      "Epoch 3 \tLoss: 0.170758 \tLoss (val): 0.152220\n",
      "Epoch 4 \tLoss: 0.168293 \tLoss (val): 0.152512\n",
      "Progress: 67.74%\n",
      "Epoch 0 \tLoss: 0.186548 \tLoss (val): 0.111930\n",
      "Epoch 1 \tLoss: 0.182779 \tLoss (val): 0.111552\n",
      "Epoch 2 \tLoss: 0.181247 \tLoss (val): 0.111793\n",
      "Epoch 3 \tLoss: 0.179969 \tLoss (val): 0.112376\n",
      "Epoch 4 \tLoss: 0.178966 \tLoss (val): 0.112591\n",
      "Progress: 70.97%\n",
      "Epoch 0 \tLoss: 0.168250 \tLoss (val): 0.230736\n",
      "Epoch 1 \tLoss: 0.165576 \tLoss (val): 0.234997\n",
      "Epoch 2 \tLoss: 0.163760 \tLoss (val): 0.238081\n",
      "Epoch 3 \tLoss: 0.162221 \tLoss (val): 0.237958\n",
      "Epoch 4 \tLoss: 0.159657 \tLoss (val): 0.242836\n",
      "Progress: 74.19%\n",
      "Epoch 0 \tLoss: 2.337735 \tLoss (val): 0.234388\n",
      "Epoch 1 \tLoss: 2.305394 \tLoss (val): 0.238322\n",
      "Epoch 2 \tLoss: 2.294284 \tLoss (val): 0.239035\n",
      "Epoch 3 \tLoss: 2.285124 \tLoss (val): 0.238718\n",
      "Epoch 4 \tLoss: 2.279865 \tLoss (val): 0.241186\n",
      "Progress: 77.42%\n",
      "Epoch 0 \tLoss: 0.390838 \tLoss (val): 0.209729\n",
      "Epoch 1 \tLoss: 0.384477 \tLoss (val): 0.204270\n",
      "Epoch 2 \tLoss: 0.378864 \tLoss (val): 0.201528\n",
      "Epoch 3 \tLoss: 0.375792 \tLoss (val): 0.201055\n",
      "Epoch 4 \tLoss: 0.373112 \tLoss (val): 0.200936\n",
      "Progress: 80.65%\n",
      "Append test data\n",
      "Progress: 83.87%\n",
      "Append test data\n",
      "Progress: 87.10%\n",
      "Append test data\n",
      "Progress: 90.32%\n",
      "Append test data\n",
      "Progress: 93.55%\n",
      "Append test data\n",
      "Progress: 96.77%\n",
      "Append test data\n",
      "Progress: 100.00%\n",
      "Append test data\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "model = StockModel(input_size)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)\n",
    "test_data_x = pd.DataFrame()\n",
    "test_data_y = pd.DataFrame()\n",
    "for i in range(len(files_x)):\n",
    "    print(\"Current file: \" + files_x[i])\n",
    "    total_rows = sum(1 for row in open(files_x[i], 'r'))\n",
    "    number_of_loops = int(total_rows/chunksize)\n",
    "    print(\"Number of chunks: \" + str(number_of_loops))\n",
    "    current_loop = 0\n",
    "    with pd.read_csv(files_x[i], sep=\";\", dtype=\"float32\", usecols = [j for j in range(input_size)], chunksize=chunksize) as reader_x,\\\n",
    "    pd.read_csv(files_y[i], sep=\";\", dtype=\"float32\", converters = {'ts': int}, chunksize=chunksize) as reader_y:\n",
    "        for chunk_x, chunk_y in zip(reader_x, reader_y):\n",
    "            print(\"Progress: \" + \"{:.2f}\".format(100 * current_loop/number_of_loops) + \"%\")\n",
    "            x_data = chunk_x\n",
    "            y_data = chunk_y\n",
    "            if(current_loop < data_split_ratio * number_of_loops):\n",
    "                y_data = y_data[y_column]\n",
    "                train_chunk(model, loss_fn, optimizer, nbr_epochs, x_data, y_data, data_split_ratio, batch_size)\n",
    "            else:\n",
    "                print(\"Append test data\")\n",
    "                test_data_x = test_data_x.append(x_data)\n",
    "                test_data_y = test_data_y.append(y_data)\n",
    "            current_loop+=1\n",
    "\n",
    "test_data_x = torch.tensor(test_data_x.values, dtype=torch.float32)\n",
    "test_data_y_norm = torch.tensor(test_data_y[y_column].values, dtype=torch.float32)\n",
    "test_data = TensorDataset(test_data_x, test_data_y_norm)\n",
    "#test_data_y = test_data_y[y_column]\n",
    "#test_data = list(zip(test_data_x, test_data_y))\n",
    "#for i in range(len(files_x)):\n",
    "#    x_data = pd.read_csv(files_x[i], sep=\";\", dtype=\"float32\", usecols = [j for j in range(input_size)])\n",
    "#    y_data = pd.read_csv(files_y[i], sep=\";\", dtype=\"float32\", converters = {'ts': int})\n",
    "#    y_data = y_data[y_column]\n",
    "#    print(x_data.shape)\n",
    "#    print(y_data.shape)\n",
    "#    x_data.head()\n",
    "#    y_data.head()\n",
    "#    print(files_x[i])\n",
    "#    train_data, dev_data, test_data = splitData(x_data, y_data, data_split_ratio)\n",
    "#    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "#    dev_data_loader = DataLoader(dev_data, batch_size=batch_size)\n",
    "#    model = model.to(device)\n",
    "#    train_model(model, train_data_loader, dev_data_loader, loss_fn, optimizer, nbr_epochs, batch_size)\n",
    "#    del [[x_data, y_data, train_data, dev_data, train_data_loader, dev_data_loader]]\n",
    "#    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/anaconda3/envs/gtx/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([389])) that is different to the input size (torch.Size([389, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "_, preds = evaluate_model(test_data_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_norm(x, min_x, max_x):\n",
    "    max_x = max_x\n",
    "    min_x = min_x\n",
    "    return (x*(max_x-min_x+0.0001)+min_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500000   -0.6314\n",
      "2500001   -0.6314\n",
      "2500002   -0.6314\n",
      "2500003   -0.6314\n",
      "2500004   -0.6314\n",
      "Name: 600s, dtype: float32\n",
      "0   -0.000551\n",
      "1   -0.000551\n",
      "2   -0.000551\n",
      "3   -0.000551\n",
      "4   -0.000551\n",
      "dtype: float32\n",
      "0    141.679783\n",
      "1    141.679783\n",
      "2    141.679783\n",
      "3    141.679783\n",
      "4    141.679783\n",
      "dtype: float64\n",
      "0    141.439994\n",
      "1    141.439994\n",
      "2    141.439994\n",
      "3    141.439994\n",
      "4    141.439994\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_norm = test_data_y['min'].reset_index(drop=True)\n",
    "max_norm = test_data_y['max'].reset_index(drop=True)\n",
    "\n",
    "np_preds = np.array(preds, dtype='float32').squeeze()\n",
    "series_preds = pd.Series(np_preds, dtype='float32')\n",
    "print(test_data_y[y_column].head())\n",
    "print(series_preds.head())\n",
    "items_plot = from_norm(test_data_y[y_column].reset_index(drop=True), min_norm, max_norm)\n",
    "pred_plot = from_norm(series_preds, min_norm, max_norm)\n",
    "print(pred_plot.head())\n",
    "print(items_plot.head())\n",
    "plt.plot(list(range(len(pred_plot))), pred_plot)\n",
    "plt.plot(list(range(len(items_plot))), items_plot)\n",
    "#plt.plot(list(range(len(pred_plot))), preds)\n",
    "#plt.plot(list(range(len(items_plot))), test_data_y[y_column])\n",
    "axes = plt.gca()\n",
    "#axes.set_ylim([20000,25000])\n",
    "#axes.set_xlim([260000,261200])\n",
    "axes.set_xlim([100000,120000])\n",
    "#axes.set_xlim([140000,160000])\n",
    "\n",
    "#xes.set_xlim([540000,560000])\n",
    "#axes.set_xlim([610000,660000])\n",
    "#axes.set_xlim([660000,660100])\n",
    "#axes.set_ylim([157,158])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
