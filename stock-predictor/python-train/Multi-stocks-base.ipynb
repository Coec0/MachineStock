{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "input_size=500\n",
    "batch_size=256\n",
    "nbr_epochs=5\n",
    "data_split_ratio=0.8\n",
    "chunksize = 100000\n",
    "lr = 0.001\n",
    "y_column = \"600s\"\n",
    "files_x = [\"data/x_SEB_A_500_p.csv\",\n",
    "           \"data/x_Nordea_Bank_Abp_500_p.csv\"]\n",
    "files_y = [\"data/y_SEB_A_500.csv\",\n",
    "           \"data/y_Nordea_Bank_Abp_500.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(xs, ys, trainRatio):\n",
    "    t = round(len(xs)*trainRatio)\n",
    "    \n",
    "    train_data_x = torch.tensor(xs[:t].values, dtype=torch.float32)\n",
    "    train_data_y = torch.tensor(ys[:t].values, dtype=torch.float32)\n",
    "    \n",
    "    dev_data_x = torch.tensor(xs[t:].values, dtype=torch.float32)\n",
    "    dev_data_y = torch.tensor(ys[t:].values, dtype=torch.float32)\n",
    "    \n",
    "    return TensorDataset(train_data_x, train_data_y), TensorDataset(dev_data_x, dev_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_unit = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device(\"cpu\")\n",
    "#compute_unit = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtype = torch.FloatTensor\n",
    "dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1000).type(dtype)\n",
    "        self.fc2 = nn.Linear(1000, 2000).type(dtype)\n",
    "        self.fc3 = nn.Linear(2000, 1000).type(dtype)\n",
    "        self.fc4 = nn.Linear(1000, 500).type(dtype)\n",
    "        self.fc5 = nn.Linear(500, 100).type(dtype)\n",
    "        self.fc6 = nn.Linear(100, 20).type(dtype)\n",
    "        self.fc7 = nn.Linear(20, 1).type(dtype)\n",
    "        \n",
    "        self.drop_layer = nn.Dropout(p=0.010)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        fc1 = self.fc1(x)\n",
    "        x = F.relu(fc1)\n",
    "        x = self.drop_layer(x)\n",
    "        #print(fc1)\n",
    "        #print(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop_layer(x)\n",
    "        #print(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop_layer(x)\n",
    "        #x = self.drop_layer(x)\n",
    "        #print(x)\n",
    "        x= F.relu(self.fc4(x))\n",
    "        #print(y)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.drop_layer(x)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        y = F.relu(self.fc7(x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data, model, loss_fn):\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data:\n",
    "            y = y.squeeze().type(dtype)\n",
    "            x = x.squeeze().type(dtype)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            losses.append(loss.item())\n",
    "            predictions.extend(pred.tolist())\n",
    "        avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return avg_loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data_loader, dev_data_loader, loss_fn, optimizer, epochrange, batchsize):\n",
    "    for epoch in range(epochrange):\n",
    "        losses = []\n",
    "        n_correct = 0\n",
    "        model.train()\n",
    "        for x, y in train_data_loader:\n",
    "            y = y.type(dtype)\n",
    "            x = x.type(dtype)\n",
    "            \n",
    "            pred = model(x)\n",
    "\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()    \n",
    "\n",
    "        # Compute accuracy and loss in the entire training set\n",
    "        train_avg_loss = sum(losses)/len(losses)    \n",
    "        \n",
    "        dev_avg_loss,_ = evaluate_model(dev_data_loader, model, loss_fn)\n",
    "        \n",
    "        # Display metrics\n",
    "        display_str = 'Epoch {} '\n",
    "        display_str += '\\tLoss: {:.3f} '\n",
    "        display_str += '\\tLoss (val): {:.3f}'\n",
    "        print(display_str.format(epoch, train_avg_loss, dev_avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_chunk(model, loss_fn, optimizer, nbr_epochs, x_data, y_data, data_split_ratio, batch_size):\n",
    "    train_data, dev_data = splitData(x_data, y_data, data_split_ratio)\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    dev_data_loader = DataLoader(dev_data, batch_size=batch_size)\n",
    "    train_model(model, train_data_loader, dev_data_loader, loss_fn, optimizer, nbr_epochs, batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file: data/x_SEB_A_500_p.csv\n",
      "Number of chunks: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kajen\\.conda\\envs\\titantesttest\\lib\\site-packages\\ipykernel_launcher.py:16: ParserWarning: Both a converter and dtype were specified for column ts - only the converter will be used\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.00%\n",
      "Epoch 0 \tLoss: 97.071 \tLoss (val): 0.378\n",
      "Epoch 1 \tLoss: 3.371 \tLoss (val): 0.128\n"
     ]
    }
   ],
   "source": [
    "#Start training\n",
    "model = StockModel(input_size)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model = model.to(device)\n",
    "test_data_x = pd.DataFrame()\n",
    "test_data_y = pd.DataFrame()\n",
    "for i in range(len(files_x)):\n",
    "    print(\"Current file: \" + files_x[i])\n",
    "    total_rows = sum(1 for row in open(files_x[i], 'r'))\n",
    "    number_of_loops = int(total_rows/chunksize)\n",
    "    print(\"Number of chunks: \" + str(number_of_loops))\n",
    "    current_loop = 0\n",
    "    with pd.read_csv(files_x[i], sep=\";\", dtype=\"float32\", usecols = [j for j in range(input_size)], chunksize=chunksize) as reader_x,\\\n",
    "    pd.read_csv(files_y[i], sep=\";\", dtype=\"float32\", converters = {'ts': int}, chunksize=chunksize) as reader_y:\n",
    "        for chunk_x, chunk_y in zip(reader_x, reader_y):\n",
    "            print(\"Progress: \" + \"{:.2f}\".format(100 * current_loop/number_of_loops) + \"%\")\n",
    "            y_data = chunk_y[y_column]\n",
    "            x_data = chunk_x\n",
    "            \n",
    "            if(current_loop < data_split_ratio * number_of_loops):\n",
    "                train_chunk(model, loss_fn, optimizer, nbr_epochs, x_data, y_data, data_split_ratio, batch_size)\n",
    "            else:\n",
    "                print(\"Append test data\")\n",
    "                test_data_x = test_data_x.append(x_data)\n",
    "                test_data_y = test_data_y.append(y_data)\n",
    "            current_loop+=1\n",
    "\n",
    "#for i in range(len(files_x)):\n",
    "#    x_data = pd.read_csv(files_x[i], sep=\";\", dtype=\"float32\", usecols = [j for j in range(input_size)])\n",
    "#    y_data = pd.read_csv(files_y[i], sep=\";\", dtype=\"float32\", converters = {'ts': int})\n",
    "#    y_data = y_data[y_column]\n",
    "#    print(x_data.shape)\n",
    "#    print(y_data.shape)\n",
    "#    x_data.head()\n",
    "#    y_data.head()\n",
    "#    print(files_x[i])\n",
    "#    train_data, dev_data, test_data = splitData(x_data, y_data, data_split_ratio)\n",
    "#    train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "#    dev_data_loader = DataLoader(dev_data, batch_size=batch_size)\n",
    "#    model = model.to(device)\n",
    "#    train_model(model, train_data_loader, dev_data_loader, loss_fn, optimizer, nbr_epochs, batch_size)\n",
    "#    del [[x_data, y_data, train_data, dev_data, train_data_loader, dev_data_loader]]\n",
    "#    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds = evaluate_model(test_data, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_borpi = list(zip(*test_data))\n",
    "print(preds[0])\n",
    "items_plot = [y_data_borpi[1][t] for t in range(len(y_data_borpi[1]))]\n",
    "plt.plot(list(range(len(preds))), preds)\n",
    "plt.plot(list(range(len(items_plot))), items_plot)\n",
    "axes = plt.gca()\n",
    "#axes.set_ylim([145,170])\n",
    "#axes.set_xlim([250000,265000])\n",
    "#axes.set_xlim([100000,120000])\n",
    "#axes.set_xlim([140000,160000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
