{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "supreme-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch, math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aging-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "interesting-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../python-docker/Swedbank_A/x_Swedbank_A_transformer.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "accredited-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"] = df[\"close\"].shift(-1)\n",
    "df = df[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "catholic-three",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>cos_hour</th>\n",
       "      <th>sin_min</th>\n",
       "      <th>cos_min</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55031</th>\n",
       "      <td>150.24</td>\n",
       "      <td>150.22</td>\n",
       "      <td>79.8246</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.895163</td>\n",
       "      <td>0.445738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>150.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55032</th>\n",
       "      <td>150.24</td>\n",
       "      <td>150.38</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.895163</td>\n",
       "      <td>0.445738</td>\n",
       "      <td>0.104528</td>\n",
       "      <td>0.994522</td>\n",
       "      <td>150.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55033</th>\n",
       "      <td>150.36</td>\n",
       "      <td>150.38</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.895163</td>\n",
       "      <td>0.445738</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>150.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55034</th>\n",
       "      <td>150.40</td>\n",
       "      <td>150.58</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.895163</td>\n",
       "      <td>0.445738</td>\n",
       "      <td>0.309017</td>\n",
       "      <td>0.951057</td>\n",
       "      <td>150.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55035</th>\n",
       "      <td>150.58</td>\n",
       "      <td>150.80</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.895163</td>\n",
       "      <td>0.445738</td>\n",
       "      <td>0.406737</td>\n",
       "      <td>0.913545</td>\n",
       "      <td>150.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open   close      rsi    macd  sin_month  cos_month   sin_day  \\\n",
       "55031  150.24  150.22  79.8246  0.0030   0.866025        0.5  0.207912   \n",
       "55032  150.24  150.38  76.4706  0.0129   0.866025        0.5  0.207912   \n",
       "55033  150.36  150.38  76.4706  0.0112   0.866025        0.5  0.207912   \n",
       "55034  150.40  150.58  76.4706  0.0161   0.866025        0.5  0.207912   \n",
       "55035  150.58  150.80  76.4706  0.0134   0.866025        0.5  0.207912   \n",
       "\n",
       "        cos_day  sin_hour  cos_hour   sin_min   cos_min  target  \n",
       "55031  0.978148  0.895163  0.445738  0.000000  1.000000  150.38  \n",
       "55032  0.978148  0.895163  0.445738  0.104528  0.994522  150.38  \n",
       "55033  0.978148  0.895163  0.445738  0.207912  0.978148  150.58  \n",
       "55034  0.978148  0.895163  0.445738  0.309017  0.951057  150.80  \n",
       "55035  0.978148  0.895163  0.445738  0.406737  0.913545  150.88  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "meaning-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = sorted(df.index.values)  # get the times\n",
    "last_10pct = sorted(df.index.values)[-int(0.1*len(times))]  # get the last 10% of the times\n",
    "last_20pct = sorted(df.index.values)[-int(0.2*len(times))]  # get the last 20% of the times\n",
    "\n",
    "test_data = df[(df.index >= last_10pct)].values\n",
    "val_data = df[(df.index >= last_20pct) & (df.index < last_10pct)].values\n",
    "train_data = df[(df.index < last_20pct)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "artificial-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_output(window_size, data):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(window_size, len(data)-1):\n",
    "        X.append(data[i-window_size:i][:,:-1])\n",
    "        y.append([data[i-window_size:i][:,-1]])\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "angry-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = build_input_output(10, train_data)\n",
    "x_val, y_val = build_input_output(10, val_data)\n",
    "x_test, y_test = build_input_output(10, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "large-armstrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44018, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "initial-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "dev_data_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "treated-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # d_model : number of features\n",
    "    def __init__(self,feature_size=7,num_layers=3,dropout=0):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=feature_size, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, device):\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        print(mask)\n",
    "        output = self.transformer_encoder(src,mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "economic-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data_loader, dev_data_loader, loss_fn, optimizer, epochrange, batchsize, device):\n",
    "    for epoch in range(epochrange):\n",
    "        losses = []\n",
    "        n_correct = 0\n",
    "        model.train()\n",
    "        for x, y in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y = y.type(dtype).permute(2,0,1)\n",
    "            x = x.type(dtype).transpose(0,1)\n",
    "            \n",
    "            print(x.shape)\n",
    "            pred = model(x, device)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        # Compute accuracy and loss in the entire training set\n",
    "        train_avg_loss = sum(losses)/len(losses)    \n",
    "        \n",
    "        dev_avg_loss, preds = evaluate_model(dev_data_loader, model, loss_fn, device)\n",
    "        print(preds[0])\n",
    "        \n",
    "        # Display metrics\n",
    "        display_str = 'Epoch {} '\n",
    "        display_str += '\\tLoss: {:.3f} '\n",
    "        display_str += '\\tLoss (val): {:.3f}'\n",
    "        print(display_str.format(epoch, train_avg_loss, dev_avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adverse-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data, model, loss_fn, device):\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in data:\n",
    "            y = y.type(dtype).permute(2,0,1)\n",
    "            x = x.type(dtype).transpose(0,1)\n",
    "            pred = model(x, device)\n",
    "            loss = loss_fn(pred, y)\n",
    "            losses.append(loss.item())\n",
    "            predictions.extend(pred.tolist())\n",
    "        avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return avg_loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "quiet-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 12])\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-cd0669dddb58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-cc73576073ef>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data_loader, dev_data_loader, loss_fn, optimizer, epochrange, batchsize, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\titantesttest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-368502bd6eef>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_square_subsequent_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\titantesttest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\titantesttest\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             output = self.layers[i](output, src_mask=mask,\n\u001b[1;32m--> 167\u001b[1;33m                                     src_key_padding_mask=src_key_padding_mask)\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\titantesttest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\titantesttest\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m    265\u001b[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[1;32m--> 266\u001b[1;33m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\titantesttest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\titantesttest\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[0;32m    781\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m                 attn_mask=attn_mask)\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\titantesttest\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[0;32m   3099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3100\u001b[0m     \u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3101\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3102\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3103\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "model = Transformer(feature_size=5).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_model(model, train_data_loader, dev_data_loader, loss_fn, optimizer, 10, 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "enabling-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds = evaluate_model(test_data_loader, model, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "atlantic-parallel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.81271362304688]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " [[150.8126983642578]],\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "norwegian-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_data_ = list(zip(*test_data))\n",
    "#items_plot = [y_data_borpi[1][t] for t in range(len(y_data_borpi[1]))]\n",
    "#plt.plot(list(range(len(preds))), preds)\n",
    "#plt.plot(list(range(len(items_plot))), items_plot)\n",
    "#axes = plt.gca()\n",
    "#axes.set_ylim([145,170])\n",
    "#axes.set_xlim([320000,325000])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-freeware",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
